# Self-Driving Task Execution System

## Purpose
The purpose of this system is to execute a series of tasks iteratively. The system follows a cyclical process of reading, executing, and regenerating the next actionable step.

## How It Works
1. **Initialization**: The system starts by reading the initial `next_step.md` file, which contains the current actionable step and the protocol for the next iteration.
2. **Execution**: The system executes the current actionable step detailed in `next_step.md`.
3. **Regeneration**: Upon completion of the current step, the system determines the next specific actionable step based on the overall plan or the outcome of the current step. It then creates (or overwrites) a new `next_step.md` file with the updated current actionable step and the next iteration protocol.
4. **Cycle Continuation**: The system initiates a new task to read the updated `next_step.md` file, execute the new current actionable step, and continue the cycle.

## Cyclical Nature
The system operates in a continuous loop, where each iteration involves:
- Reading the current actionable step from `next_step.md`.
- Executing that step.
- Determining the next actionable step.
- Updating `next_step.md` with the new step and protocol.
- Initiating a new task to continue the cycle.

This cyclical process ensures that tasks are executed iteratively and systematically, allowing for efficient and organized task management.

### Translation program

Project Title: Iterative Translation Assistant (Flask Edition)

README.md

# Iterative Translation Assistant (Flask Edition)

## Description

The Iterative Translation Assistant is a web-based tool with a Python/Flask backend, designed to assist in translating sentences with the support of a Language Model (LLM). Instead of a single translation, the system presents four alternatives generated by the LLM, highlighting the differences between them on the client-side. The user can then edit any of the alternatives or accept the one that best suits their needs.

The core feature is iterative learning: each accepted translation is saved in the user's browser `localStorage`. These saved translations are then sent back to the server with the next translation request and used as few-shot examples in the prompt for the LLM. This aims to refine and contextualize the model's suggestions over time, adapting them to the user's style and preferences.

## Features

*   **Web Interface:** Frontend built with HTML, CSS, and JavaScript.
*   **Flask Backend:** Python/Flask server to handle translation requests.
*   **Sentence Input:** Field for the user to enter the original sentence.
*   **LLM Communication:**
    *   The Flask backend calls a `completion` function from an `LLM` class (provided in `llm.py`) to get translations.
    *   Requests 4 translation alternatives from the LLM.
*   **Difference Highlighting (Client-Side):** Differences between translation alternatives are visually highlighted in the browser using JavaScript.
*   **Direct Editing:** "Edit" button next to each alternative.
*   **Translation Acceptance:** "Accept" button to select the best translation.
*   **Local Persistence (Client-Side):** Accepted translations (original-translated pair) are saved in the browser's `localStorage`.
*   **Continuous Prompt Improvement (Context via Client):** Saved translations from `localStorage` are sent to the backend with each new translation request. The backend uses them to build a contextualized prompt for the LLM.

## File Structure

*   `app.py`: Main Flask application, routes, and backend logic.
*   `llm.py`: **(Your existing file)** Contains the `LLM` class with the `completion` method.
*   `templates/`:
    *   `index.html`: Main HTML structure of the web page.
*   `static/`:
    *   `css/style.css`: Styles for the user interface.
    *   `js/script.js`: Client-side JavaScript logic (UI interaction, AJAX calls to backend, DOM manipulation).
    *   `js/utils.js`: JavaScript utility functions (e.g., difference highlighting, localStorage manipulation).
*   `requirements.txt`: Python dependencies (Flask, etc.).
*   `.gitignore`: To ignore unnecessary files in Git.
*   `README.md`: This file.

## How to Use

1.  **Prerequisites:**
    *   Python 3.x installed.
    *   An `llm.py` file in the project root with your `LLM` class and `completion` method.
        *   Expected structure in `llm.py`:
            ```python
            class LLM:
                def __init__(self, args_if_any):
                    # Initialize your model
                    pass

                def completion(self, prompt: str, num_alternatives: int = 1) -> list[str]:
                    """
                    Generates 'num_alternatives' completions for the given prompt.
                    Returns a list of strings.
                    """
                    # Your logic to call the LLM and get 'num_alternatives'
                    # Example mock:
                    # alternatives = []
                    # for i in range(num_alternatives):
                    #    alternatives.append(f"Mock translation {i+1} of: {prompt.split(':')[-1].strip()}")
                    # return alternatives
                    raise NotImplementedError("Implement the completion method in your LLM class")
            ```

2.  **Setup:**
    *   Clone the repository.
    *   Create and activate a virtual environment:
        ```bash
        python -m venv venv
        source venv/bin/activate  # On Windows: venv\Scripts\activate
        ```
    *   Install dependencies:
        ```bash
        pip install -r requirements.txt
        ```
    *   Ensure your `llm.py` is correctly configured (API keys, model paths, etc.).

3.  **Run the Application:**
    ```bash
    flask run
    # Or python app.py (if __main__ is configured)
    ```
4.  Open your browser and navigate to `http://127.0.0.1:5000/`.
5.  Enter the sentence you want to translate.
6.  Saved translations from `localStorage` will be automatically sent with the request.
7.  Analyze the 4 alternatives, edit, or accept.
8.  Accepted translations are saved in `localStorage` for future requests.

## Technologies

*   **Backend:** Python, Flask
*   **Frontend:** HTML, CSS, JavaScript (Vanilla JS)
*   **Frontend-Backend Communication:** AJAX (`fetch` API)
*   **LLM Interface:** Your `llm.py` file
*   **Context Storage:** Browser `localStorage`.
*   **Difference Highlighting:** JavaScript (can use a library like `diff-match-patch` or custom implementation).

## Potential Future Improvements

*   Support for multiple source/target languages.
*   Option to export/import saved translations from `localStorage`.
*   Configuration of LLM parameters (temperature, etc.) via the UI.
*   User authentication to save translations server-side (instead of `localStorage`).
*   Enhanced error handling.

Implementation Plan
1. List of Files and Main Functions/Classes

a) app.py (Flask Application)
* Imports: Flask, render_template, request, jsonify
* Import: from llm import LLM (assuming your file is llm.py)
* Global: app = Flask(__name__), llm_instance = LLM() (initialize your LLM class here)
* Route / (GET):
* @app.route('/')
* def index():
* return render_template('index.html')
* Route /translate (POST):
* @app.route('/translate', methods=['POST'])
* def translate_text():
* data = request.get_json()
* original_sentence = data.get('original_sentence')
* saved_translations_context = data.get('saved_translations', []) (list of objects {original, translated})
* if not original_sentence: return jsonify({'error': 'No sentence provided'}), 400
* prompt = build_prompt_with_context(original_sentence, saved_translations_context)
* LLM Call:
* try:
* alternatives = llm_instance.completion(prompt, num_alternatives=4)
* if not isinstance(alternatives, list) or len(alternatives) != 4:
* # Log the issue if LLM doesn't return 4 alternatives as a list
* # Consider a fallback or more robust error handling
* return jsonify({'error': 'LLM did not return the expected format'}), 500
* return jsonify({'alternatives': alternatives})
* except Exception as e:
* # Log the error (e.g., app.logger.error(f"LLM error: {e}"))
* return jsonify({'error': str(e)}), 500
* Helper Function build_prompt_with_context(original_sentence, saved_translations):
* def build_prompt_with_context(original_sentence, saved_translations):
* context_str = ""
* for item in saved_translations:
* context_str += f"Original sentence: {item['original']}\nAccepted translation: {item['translated']}\n\n"
* prompt = f"{context_str}Considering the examples above, translate the following sentence into 4 different variations, showing subtle nuances where possible.\n\nOriginal sentence: {original_sentence}\n\nTranslation alternatives:"
* return prompt
* (Optional) if __name__ == '__main__': app.run(debug=True)

b) llm.py (Your Existing File - Adapt if necessary)
* class LLM:
* def __init__(self, ...): (Your initialization)
* def completion(self, prompt: str, num_alternatives: int = 1) -> list[str]:
* Important: This method MUST be able to return a list of num_alternatives strings.
* If your current completion method only returns one string, you'll need to call it num_alternatives times in a loop here, or modify the LLM logic to support an n parameter (like in OpenAI APIs).
* Example adaptation if completion only returns one string:
python # class LLM: # def _single_completion(self, prompt: str) -> str: # # your original logic that returns a single string # # return "some translation" # # def completion(self, prompt: str, num_alternatives: int = 1) -> list[str]: # if num_alternatives == 1: # return [self._single_completion(prompt)] # else: # # Caution: Calling the same prompt multiple times might yield identical results # # if the model is deterministic or has low temperature. # # You might need to adjust temperature/top_p or slightly vary the prompt # # for each call if the LLM API doesn't support 'n' directly. # results = [] # for i in range(num_alternatives): # # Slightly vary prompt if needed to encourage diverse outputs # variant_prompt = prompt + f" (variation {i + 1})" # results.append(self._single_completion(variant_prompt)) # return results

c) templates/index.html (UI Structure)
* Reference CSS: <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
* Reference JS: <script src="{{ url_for('static', filename='js/utils.js') }}" defer></script>
<script src="{{ url_for('static', filename='js/script.js') }}" defer></script>
* HTML Elements:
* <textarea id="original-text-input"></textarea>
* <button id="translate-button">Translate</button>
* <div id="alternatives-container"></div>
* <div id="error-message" style="color: red;"></div> (To display backend errors)

d) static/css/style.css (Styling)
* Use bootstrap and use this file only for basic styling 

e) static/js/script.js (Client-Side Logic)
* document.addEventListener('DOMContentLoaded', () => { ... });
* Inside DOMContentLoaded (or an init() function):
* const translateButton = document.getElementById('translate-button');
* translateButton.addEventListener('click', handleTranslateClick);
* async function handleTranslateClick():
* const originalText = document.getElementById('original-text-input').value;
* const savedTranslations = loadSavedTranslations(); // From utils.js
* const errorMessageDiv = document.getElementById('error-message');
* errorMessageDiv.textContent = ''; // Clear previous errors
* try {
* const response = await fetch('/translate', {
* method: 'POST',
* headers: { 'Content-Type': 'application/json' },
* body: JSON.stringify({ original_sentence: originalText, saved_translations: savedTranslations })
* });
* if (!response.ok) {
* const errorData = await response.json();
* throw new Error(errorData.error || \Server error: ${response.status}`);*}*const data = await response.json();*if (data.alternatives) {*displayAlternatives(originalText, data.alternatives);*}*} catch (error) {*console.error("Error translating:", error);*errorMessageDiv.textContent = error.message;*}* **displayAlternatives(originalSentence, alternativesArray):** * Clears#alternatives-container. * For each translation: * UseshighlightDifferences()fromutils.jsto generate HTML with highlights. * Creates "Edit" and "Accept" buttons. * Adds event listeners (handleEditClick,handleAcceptClick). * **handleEditClick(index, currentTextElement, alternativeDiv):** (Allows inline editing) * **handleSaveEditClick(index, textareaElement, originalSentence, alternativeDiv):** (Saves edit, then callshandleAcceptClick) * **handleAcceptClick(originalSentence, acceptedTranslationText):** * CallssaveTranslationPair(originalSentence, acceptedTranslationText)fromutils.js`.
* Provides user feedback.

f) static/js/utils.js (JavaScript Utility Functions)
* highlightDifferences(baseString, comparisonStringsArray):
* Uses diff-match-patch (include the library in static/js or via CDN) or custom logic.
* Compares each string in comparisonStringsArray against the first (baseString = comparisonStringsArray[0]) or performs pairwise comparisons.
* saveTranslationPair(original, translated): Saves to localStorage.
* loadSavedTranslations(): Loads from localStorage. Returns an array of {original, translated} objects.

g) requirements.txt
Flask # Other dependencies your llm.py might have

2. UML Overview (Simplified Component Diagram with Flask)
graph LR
    BrowserClient["Browser (HTML, CSS, JS - script.js, utils.js)"] -- HTTP POST /translate (JSON) --> FlaskServer["Flask App (app.py)"]
    BrowserClient -- HTTP GET / --> FlaskServer
    FlaskServer -- Render template --> BrowserClient

    FlaskServer -- Calls method --> LLM_Module["LLM Module (llm.py - Class LLM)"]
    LLM_Module -- Interacts with --> ActualLLM["Actual LLM Service/Model"]

    BrowserClient -- Reads/Writes --> LocalStorage["Browser LocalStorage"]

    subgraph "Client-Side"
        BrowserClient
        LocalStorage
    end

    subgraph "Server-Side (Python)"
        FlaskServer
        LLM_Module
    end
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Mermaid
IGNORE_WHEN_COPYING_END

Main Data Flow:

User opens http://127.0.0.1:5000/ in the BrowserClient.

FlaskServer (app.py) serves index.html.

script.js in the BrowserClient initializes.

User enters text and clicks "Translate".

script.js:

Gets the original text.

Calls loadSavedTranslations() from utils.js to get context from LocalStorage.

Sends a fetch POST request to /translate on the FlaskServer with { "original_sentence": "...", "saved_translations": [...] }.

FlaskServer (app.py, /translate route):

Receives the JSON data.

Calls build_prompt_with_context() to create the full prompt.

Calls llm_instance.completion(prompt, num_alternatives=4) from the LLM_Module.

LLM_Module interacts with the ActualLLM and returns 4 string alternatives.

FlaskServer sends the alternatives back as JSON: {'alternatives': ["alt1", "alt2", ...]}.

script.js in the BrowserClient (in the then block of the fetch):

Receives the alternatives.

Calls displayAlternatives().

displayAlternatives() uses utils.js#highlightDifferences() to highlight differences among alternatives in the DOM.

User clicks "Accept" or "Edit" -> "Save".

script.js calls utils.js#saveTranslationPair() to save the {original, accepted} pair in LocalStorage.

For the next translation, the cycle restarts from step 4, but now with more context from LocalStorage.